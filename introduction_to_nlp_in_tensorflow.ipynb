{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What we're going to cover\n",
    "\n",
    "- Downloading a text dataset\n",
    "- Visualizing text data\n",
    "- Converting text into numbers using tokenization\n",
    "- Turning our tokenized text into an embedding\n",
    "- Modelling a text dataset\n",
    "    - Starting with a baseline (TF-IDF)\n",
    "    - Building several deep learning text models\n",
    "        - Dense, LSTM, GRU, Conv1D, Transfer learning\n",
    "- Comparing the performance of each our models\n",
    "- Combining our models into an ensemble\n",
    "- Saving and loading a trained model\n",
    "- Find the most wrong predictions"
   ],
   "id": "115c03481df181a5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-16T22:08:13.415411Z",
     "start_time": "2025-12-16T22:08:13.410420Z"
    }
   },
   "source": [
    "import datetime\n",
    "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last run (end-to-end): 2025-12-16 23:08:13.413418\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check for GPU",
   "id": "70a284f63ea0691f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:08:51.233605Z",
     "start_time": "2025-12-16T22:08:51.127618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ],
   "id": "755f9f137b67704d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA T500 (UUID: GPU-2e388ea4-5136-87e9-1d14-8e74849e1774)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get helper functions",
   "id": "936bf2bf50b81351"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:10:49.628949Z",
     "start_time": "2025-12-16T22:10:49.394802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download helper functions script\n",
    "#!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\"\n",
    "urllib.request.urlretrieve(url, \"helper_functions.py\")\n"
   ],
   "id": "d69b5a81110f0c60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('helper_functions.py', <http.client.HTTPMessage at 0x2538e681c10>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:17:39.992247Z",
     "start_time": "2025-12-16T22:17:38.316925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ],
   "id": "2f8ad4f16fafa893",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download a text dataset from Kaggle",
   "id": "9b66333c08a08b94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:25:42.044327Z",
     "start_time": "2025-12-16T22:25:41.028319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download data (same as from Kaggle)import urllib.request\n",
    "url = \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "urllib.request.urlretrieve(url, \"nlp_getting_started.zip\")\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"nlp_getting_started.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"nlp_getting_started\")  # folder where files will be extracted\n"
   ],
   "id": "7385ffdaad763b21",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing a text dataset",
   "id": "3e0838c339179df2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:30:58.147031Z",
     "start_time": "2025-12-16T22:30:58.094029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./nlp_getting_started/train.csv\")\n",
    "test_df = pd.read_csv(\"./nlp_getting_started/test.csv\")\n",
    "train_df.head()"
   ],
   "id": "3bb5425bcf1300ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:31:25.434344Z",
     "start_time": "2025-12-16T22:31:25.417516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ],
   "id": "b073a699cff78374",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:31:53.645207Z",
     "start_time": "2025-12-16T22:31:53.636109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ],
   "id": "be78394f8c0abc8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:32:02.956357Z",
     "start_time": "2025-12-16T22:32:02.945410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ],
   "id": "96741c77ee704f73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we have two target values, we're dealing with a binary classification problem.\n",
    "\n",
    "It's fairly balanced too, about 60% negative class (target = 0) and 40% positive class (target = 1).\n",
    "\n",
    "Where,\n",
    "- 1 = a real disaster Tweet\n",
    "- 0 = not a real disaster Tweet"
   ],
   "id": "d7aca07263302cae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:33:17.940672Z",
     "start_time": "2025-12-16T22:33:17.936503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ],
   "id": "84bc8acfca08d603",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.",
   "id": "9fabae73ab411a6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:33:59.373870Z",
     "start_time": "2025-12-16T22:33:59.364914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "  _, text, target = row\n",
    "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"---\\n\")"
   ],
   "id": "1905fd2334bbc15b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@chrisroth98 @chaselabsports in an emergency situation late in the year. Not as a plan in camp\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@onihimedesu the whole city is trapped! You can't leave the city! This was supposed to be a normal sports manga wit a love triangle (c)\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "The good thing is that the #Royals won't face a newbie in the playoffs. No real reason to panic.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Our Chemical Spill Cleanup videos will prepare you for an emergency situation in the lab. http://t.co/UMQbyRUPBd\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Yumiko jumped in surprise as the fire shot upwards into the air and exploded caught off guard~ 'woah...' She had--( @LenkaIsWaifu )\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split data into training and validation sets",
   "id": "918f32b22ff087ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:37:22.425371Z",
     "start_time": "2025-12-16T22:37:22.419461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split                                                                (train_df_shuffled[\"text\"].to_numpy(),                                                        train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                random_state=42) # random state for reproducibility"
   ],
   "id": "ea0ededc452e5edf",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:37:36.525588Z",
     "start_time": "2025-12-16T22:37:36.520381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ],
   "id": "420f57218fecdc0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:37:46.989574Z",
     "start_time": "2025-12-16T22:37:46.984578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ],
   "id": "4e3ef609a27e7260",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Converting text into numbers",
   "id": "b849efa556d072ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text vectorization (tokenization)",
   "id": "822b656ed4d9282f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T22:40:38.397407Z",
     "start_time": "2025-12-16T22:40:38.344526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6\n",
    "\n",
    "# Before TensorFlow 2.6\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
    "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ],
   "id": "7d8b62b9cbe21c84",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:02:15.630606Z",
     "start_time": "2025-12-17T22:02:15.596778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ],
   "id": "f62c37dbc921130d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:06:32.183159Z",
     "start_time": "2025-12-17T22:06:32.175195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ],
   "id": "55c2d21a0080e146",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:06:33.299136Z",
     "start_time": "2025-12-17T22:06:33.213015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "id": "e772fe14173c9ca3",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:06:34.838002Z",
     "start_time": "2025-12-17T22:06:34.826042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ],
   "id": "ba4443658d956dbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:06:36.177057Z",
     "start_time": "2025-12-17T22:06:36.165960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ],
   "id": "186c37f399ccb8d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@SwellyJetEvo Disneyland! Tacos there are bomb!      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[7866,    1, 7835,   74,   22,  108,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:06:39.528878Z",
     "start_time": "2025-12-17T22:06:39.504974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ],
   "id": "acbcec29cda44912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', np.str_('the'), np.str_('a'), np.str_('in')]\n",
      "Bottom 5 least common words: [np.str_('pages'), np.str_('paeds'), np.str_('pads'), np.str_('padres'), np.str_('paddytomlinson1')]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating an Embedding using an Embedding Layer",
   "id": "4413687c3d2597da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:07:35.546100Z",
     "start_time": "2025-12-17T22:07:35.539922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\")\n",
    "\n",
    "embedding"
   ],
   "id": "3ad0378378107622",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdsan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Embedding name=embedding_1, built=False>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:07:49.567192Z",
     "start_time": "2025-12-17T22:07:49.465167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ],
   "id": "a2219b688bbe5f08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@UniversityofLaw For the people who died in Human Experiments by Unit 731 of Japanese military http://t.co/vVPLFQv58P http://t.co/eG1fsKqBv6      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.02978376,  0.03231963,  0.04895322, ..., -0.01395314,\n",
       "         -0.01427661, -0.02229604],\n",
       "        [-0.00250782,  0.02038288, -0.03441892, ...,  0.01141378,\n",
       "          0.00399121, -0.02175549],\n",
       "        [ 0.02991045, -0.02818593,  0.03505747, ..., -0.00922094,\n",
       "         -0.00011979, -0.02742982],\n",
       "        ...,\n",
       "        [ 0.03176517, -0.01581382,  0.00212751, ..., -0.04326018,\n",
       "         -0.02403592,  0.00841372],\n",
       "        [ 0.04644514, -0.03658871,  0.04242877, ..., -0.01580913,\n",
       "          0.01060022, -0.01811805],\n",
       "        [-0.04380089,  0.01523251, -0.03760218, ...,  0.00585424,\n",
       "          0.0041052 , -0.04775751]]], shape=(1, 15, 128), dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T22:08:01.651117Z",
     "start_time": "2025-12-17T22:08:01.638765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ],
   "id": "d66371eb15fdb1ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.02978376,  0.03231963,  0.04895322, -0.04285923,  0.03993431,\n",
       "       -0.02398149,  0.01329106,  0.01308082, -0.031224  ,  0.0203524 ,\n",
       "       -0.03853511,  0.0085867 ,  0.01396158, -0.03515278,  0.02296497,\n",
       "       -0.048386  ,  0.01588663, -0.01415107,  0.00369243, -0.01403586,\n",
       "       -0.01531465,  0.0042456 ,  0.00576194,  0.03868954,  0.03752667,\n",
       "        0.01208328, -0.03930784,  0.01539869,  0.04482067, -0.04906214,\n",
       "       -0.02442045,  0.04259814,  0.03053974,  0.04593151,  0.03694005,\n",
       "       -0.01483405,  0.02810654, -0.0477473 , -0.01746254,  0.00120059,\n",
       "        0.00157058, -0.03467734, -0.0351581 ,  0.02872572, -0.04904504,\n",
       "       -0.03028265,  0.01369286, -0.02979715,  0.03430815,  0.01863444,\n",
       "        0.04671285,  0.03175483, -0.02623777,  0.03272581, -0.04553641,\n",
       "        0.04228273,  0.04206872,  0.03585744,  0.00557054, -0.04624077,\n",
       "       -0.00473021,  0.0094259 , -0.01640278, -0.02163906, -0.03275955,\n",
       "        0.01150087,  0.02737777,  0.02182075,  0.04119063, -0.03565536,\n",
       "       -0.016044  ,  0.02154243, -0.00998054,  0.00859044,  0.0393957 ,\n",
       "        0.039702  , -0.0193701 , -0.01191188,  0.01420431,  0.03088099,\n",
       "       -0.03960882,  0.04452816, -0.04840573, -0.00845807, -0.03778855,\n",
       "        0.04013319,  0.04918062, -0.0384541 ,  0.03155996,  0.01265358,\n",
       "        0.04825714,  0.03715949,  0.01594922, -0.03803184, -0.04005091,\n",
       "        0.0158111 ,  0.03145656,  0.03852272, -0.02958415,  0.01035612,\n",
       "       -0.00477409,  0.02365522,  0.00886444, -0.020114  , -0.03511567,\n",
       "        0.00790968, -0.04352807,  0.01843747,  0.0206497 , -0.01828011,\n",
       "       -0.02222741,  0.0489775 ,  0.03249866,  0.03284853, -0.04717866,\n",
       "        0.00327747, -0.00584207, -0.03676845, -0.02955087,  0.03954012,\n",
       "       -0.02886418,  0.02282893,  0.01143818, -0.04299171, -0.02698435,\n",
       "       -0.01395314, -0.01427661, -0.02229604], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e01963a0d3e9da4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
